{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do the multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the graph.\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "  # This is the coefficient before L2 regularization \n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  w = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  b = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, w) + b\n",
    "\n",
    "  # Loss.\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "  ) + beta_regul * tf.nn.l2_loss(w)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, w) + b)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph initialized\n",
      "Minibatch loss at step 0: 21.496843\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 12.6%\n",
      "Minibatch loss at step 500: 3.175725\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1000: 2.120925\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1500: 1.202325\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 2000: 1.079453\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2500: 1.068426\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 3000: 0.696453\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 87.9%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "beta_val = 1e-3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Graph initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta value: 0.0001, Test accuracy: 85.9%\n",
      "Beta value: 0.00012742749857, Test accuracy: 86.3%\n",
      "Beta value: 0.000162377673919, Test accuracy: 86.5%\n",
      "Beta value: 0.000206913808111, Test accuracy: 86.3%\n",
      "Beta value: 0.000263665089873, Test accuracy: 87.0%\n",
      "Beta value: 0.000335981828628, Test accuracy: 87.0%\n",
      "Beta value: 0.000428133239872, Test accuracy: 87.3%\n",
      "Beta value: 0.000545559478117, Test accuracy: 87.9%\n",
      "Beta value: 0.000695192796178, Test accuracy: 87.6%\n",
      "Beta value: 0.00088586679041, Test accuracy: 87.9%\n",
      "Beta value: 0.00112883789168, Test accuracy: 87.8%\n",
      "Beta value: 0.00143844988829, Test accuracy: 88.1%\n",
      "Beta value: 0.00183298071083, Test accuracy: 87.9%\n",
      "Beta value: 0.00233572146909, Test accuracy: 87.8%\n",
      "Beta value: 0.00297635144163, Test accuracy: 87.7%\n",
      "Beta value: 0.00379269019073, Test accuracy: 87.6%\n",
      "Beta value: 0.00483293023857, Test accuracy: 87.5%\n",
      "Beta value: 0.00615848211066, Test accuracy: 87.3%\n",
      "Beta value: 0.00784759970351, Test accuracy: 87.2%\n",
      "Beta value: 0.01, Test accuracy: 87.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1095a2780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the graph with a series of beta values.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "beta_vals = np.logspace(-4, -2, 20)\n",
    "accuracy_vals = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "      _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    print(\"Beta value: %s, Test accuracy: %.1f%%\" % (beta_val, accuracy(test_prediction.eval(), test_labels)))\n",
    "    accuracy_vals.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "plt.semilogx(beta_vals, accuracy_vals)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's do the one-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the graph.\n",
    "\n",
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # This is the coefficient before L2 regularization \n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    ) + beta_regul * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Initialized\n",
      "Minibatch loss at step 0: 702.610718\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 22.9%\n",
      "Minibatch loss at step 500: 200.651917\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1000: 115.667305\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1500: 68.558823\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2000: 42.423004\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2500: 25.454895\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3000: 15.426962\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.9%\n",
      "Test accuracy: 92.6%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "beta_val = 1e-3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Graph initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta value: 0.0001, Test accuracy: 88.2%\n",
      "Beta value: 0.00012742749857, Test accuracy: 88.6%\n",
      "Beta value: 0.000162377673919, Test accuracy: 89.3%\n",
      "Beta value: 0.000206913808111, Test accuracy: 89.4%\n",
      "Beta value: 0.000263665089873, Test accuracy: 89.1%\n",
      "Beta value: 0.000335981828628, Test accuracy: 89.7%\n",
      "Beta value: 0.000428133239872, Test accuracy: 90.2%\n",
      "Beta value: 0.000545559478117, Test accuracy: 90.7%\n",
      "Beta value: 0.000695192796178, Test accuracy: 91.5%\n",
      "Beta value: 0.00088586679041, Test accuracy: 92.1%\n",
      "Beta value: 0.00112883789168, Test accuracy: 92.3%\n",
      "Beta value: 0.00143844988829, Test accuracy: 92.9%\n",
      "Beta value: 0.00183298071083, Test accuracy: 92.8%\n",
      "Beta value: 0.00233572146909, Test accuracy: 92.5%\n",
      "Beta value: 0.00297635144163, Test accuracy: 92.3%\n",
      "Beta value: 0.00379269019073, Test accuracy: 91.9%\n",
      "Beta value: 0.00483293023857, Test accuracy: 91.4%\n",
      "Beta value: 0.00615848211066, Test accuracy: 90.7%\n",
      "Beta value: 0.00784759970351, Test accuracy: 90.2%\n",
      "Beta value: 0.01, Test accuracy: 89.7%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1068e0e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "beta_vals = np.logspace(-4, -2, 20)\n",
    "accuracy_vals = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "      _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    print(\"Beta value: %s, Test accuracy: %.1f%%\" % (beta_val, accuracy(test_prediction.eval(), test_labels)))\n",
    "    accuracy_vals.append(accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "plt.semilogx(beta_vals, accuracy_vals)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (neural network)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the graph.\n",
    "\n",
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # This is the coefficient before L2 regularization \n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    ) + beta_regul * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Initialized\n",
      "Minibatch loss at step 0: 641.318115\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 20.7%\n",
      "Minibatch loss at step 500: 190.610229\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 1000: 115.596634\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 1500: 70.104088\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 2000: 42.515179\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 2500: 25.784170\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 3000: 15.638687\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.2%\n",
      "Test accuracy: 80.4%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "num_of_batches = 3  # We only feed 3 different batches into the NN.\n",
    "beta_val = 1e-3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Graph initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_of_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the graph.\n",
    "\n",
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # This is the coefficient before L2 regularization \n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    dropout1 = tf.nn.dropout(y1, keep_prob)\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    ) + beta_regul * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Initialized\n",
      "Minibatch loss at step 0: 765.695068\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 28.4%\n",
      "Minibatch loss at step 500: 199.755768\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1000: 114.147232\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1500: 68.156570\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2000: 41.798901\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2500: 25.338974\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 3000: 15.308699\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 92.6%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "beta_val = 1e-3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Graph initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Initialized\n",
      "Minibatch loss at step 0: 641.948059\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 31.4%\n",
      "Minibatch loss at step 500: 190.845871\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 1000: 115.739418\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 1500: 70.190788\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 2000: 42.567680\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 2500: 25.815784\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 3000: 15.657446\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.2%\n",
      "Test accuracy: 80.6%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "num_of_batches = 3  # We only feed 3 different batches into the NN.\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Graph initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_of_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size1 = 1024\n",
    "hidden_size2 = 512\n",
    "hidden_size3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # This is the coefficient before L2 regularization \n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    W1 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [image_size * image_size, hidden_size1],\n",
    "            stddev=np.sqrt(2.0 / (image_size * image_size))\n",
    "        )\n",
    "    )\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size1]))\n",
    "    \n",
    "    W2 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [hidden_size1, hidden_size2],\n",
    "            stddev=np.sqrt(2.0 / hidden_size1)\n",
    "        )\n",
    "    )\n",
    "    b2 = tf.Variable(tf.zeros([hidden_size2]))\n",
    "    \n",
    "    W3 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [hidden_size2, hidden_size3], stddev=np.sqrt(2.0 / hidden_size2)\n",
    "        )\n",
    "    )\n",
    "    b3 = tf.Variable(tf.zeros([hidden_size3]))\n",
    "    \n",
    "    W4 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "                [hidden_size3, num_labels], \n",
    "                stddev=np.sqrt(2.0 / hidden_size3)\n",
    "        )\n",
    "    )\n",
    "    b4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    layer1_train = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    dropout1 = tf.nn.dropout(layer1_train, 0.5)\n",
    "    \n",
    "    layer2_train = tf.nn.relu(tf.matmul(dropout1, W2) + b2)\n",
    "    dropout2 = tf.nn.dropout(layer2_train, 0.5)\n",
    "    \n",
    "    layer3_train = tf.nn.relu(tf.matmul(dropout2, W3) + b3)\n",
    "    dropout3 = tf.nn.dropout(layer3_train, 0.5)\n",
    "    \n",
    "    logits = tf.matmul(dropout3, W4) + b4\n",
    "    \n",
    "    # Loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    loss += beta_regul * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3) + tf.nn.l2_loss(W4))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    layer2_valid = tf.nn.relu(tf.matmul(layer1_valid, W2) + b2)\n",
    "    layer3_valid = tf.nn.relu(tf.matmul(layer2_valid, W3) + b3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(layer3_valid, W4) + b4)\n",
    "    \n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    layer2_test = tf.nn.relu(tf.matmul(layer1_test, W2) + b2)\n",
    "    layer3_test = tf.nn.relu(tf.matmul(layer2_test, W3) + b3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer3_test, W4) + b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.105654\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 15.4%\n",
      "Minibatch loss at step 500: 1.830279\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 1000: 1.320481\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1500: 0.988955\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2000: 0.989366\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2500: 0.865957\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 3000: 0.687645\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 3500: 0.674933\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 4000: 0.782368\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 4500: 0.760099\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 5000: 0.631535\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5500: 0.674094\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 6000: 0.558224\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 6500: 0.576259\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7000: 0.532238\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 7500: 0.518901\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8000: 0.681042\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 8500: 0.620936\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 9000: 0.557369\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 9500: 0.612359\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 10000: 0.698679\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 10500: 0.523059\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 11000: 0.660046\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 11500: 0.601694\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 12000: 0.625901\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12500: 0.491583\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 13000: 0.703348\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 13500: 0.766531\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 14000: 0.519640\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "beta_val = 1e-3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Graph initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
