{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do the multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the graph.\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "  # This is the coefficient before L2 regularization \n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "  ) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Initialized\n",
      "Minibatch loss at step 0: 23.441757\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 11.6%\n",
      "Minibatch loss at step 500: 3.187101\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 1000: 2.152482\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1500: 1.207876\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 2000: 1.034045\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2500: 1.107641\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 3000: 0.687937\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "beta_val = 1e-3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  print(\"Initializing\")\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta value: 0.0001, Test accuracy: 86.0%\n",
      "Beta value: 0.00012742749857, Test accuracy: 86.1%\n",
      "Beta value: 0.000162377673919, Test accuracy: 86.4%\n",
      "Beta value: 0.000206913808111, Test accuracy: 86.7%\n",
      "Beta value: 0.000263665089873, Test accuracy: 86.8%\n",
      "Beta value: 0.000335981828628, Test accuracy: 87.4%\n",
      "Beta value: 0.000428133239872, Test accuracy: 87.3%\n",
      "Beta value: 0.000545559478117, Test accuracy: 87.9%\n",
      "Beta value: 0.000695192796178, Test accuracy: 87.5%\n",
      "Beta value: 0.00088586679041, Test accuracy: 87.9%\n",
      "Beta value: 0.00112883789168, Test accuracy: 87.9%\n",
      "Beta value: 0.00143844988829, Test accuracy: 87.8%\n",
      "Beta value: 0.00183298071083, Test accuracy: 88.0%\n",
      "Beta value: 0.00233572146909, Test accuracy: 87.8%\n",
      "Beta value: 0.00297635144163, Test accuracy: 87.7%\n",
      "Beta value: 0.00379269019073, Test accuracy: 87.6%\n",
      "Beta value: 0.00483293023857, Test accuracy: 87.5%\n",
      "Beta value: 0.00615848211066, Test accuracy: 87.3%\n",
      "Beta value: 0.00784759970351, Test accuracy: 87.2%\n",
      "Beta value: 0.01, Test accuracy: 87.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEMCAYAAADEXsFmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMcYAlhCMMca4/n9CQAiGqIDu4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103c57518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the graph with a series of beta values.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "beta_vals = np.logspace(-4, -2, 20)\n",
    "accuracy_vals = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "      _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    print(\"Beta value: %s, Test accuracy: %.1f%%\" % (beta_val, accuracy(test_prediction.eval(), test_labels)))\n",
    "    accuracy_vals.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "plt.semilogx(beta_vals, accuracy_vals)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's do the one-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the graph.\n",
    "\n",
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # This is the coefficient before L2 regularization \n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    ) + beta_regul * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Initialized\n",
      "Minibatch loss at step 0: 749.316650\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 26.2%\n",
      "Minibatch loss at step 500: 207.583908\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1000: 114.615356\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1500: 68.222969\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 2000: 41.561615\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2500: 25.265743\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 3000: 15.344341\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.1%\n",
      "Test accuracy: 92.7%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  print(\"Initializing\")\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta value: 0.0001, Test accuracy: 88.2%\n",
      "Beta value: 0.00012742749857, Test accuracy: 88.6%\n",
      "Beta value: 0.000162377673919, Test accuracy: 89.3%\n",
      "Beta value: 0.000206913808111, Test accuracy: 89.4%\n",
      "Beta value: 0.000263665089873, Test accuracy: 89.1%\n",
      "Beta value: 0.000335981828628, Test accuracy: 89.7%\n",
      "Beta value: 0.000428133239872, Test accuracy: 90.2%\n",
      "Beta value: 0.000545559478117, Test accuracy: 90.7%\n",
      "Beta value: 0.000695192796178, Test accuracy: 91.5%\n",
      "Beta value: 0.00088586679041, Test accuracy: 92.1%\n",
      "Beta value: 0.00112883789168, Test accuracy: 92.3%\n",
      "Beta value: 0.00143844988829, Test accuracy: 92.9%\n",
      "Beta value: 0.00183298071083, Test accuracy: 92.8%\n",
      "Beta value: 0.00233572146909, Test accuracy: 92.5%\n",
      "Beta value: 0.00297635144163, Test accuracy: 92.3%\n",
      "Beta value: 0.00379269019073, Test accuracy: 91.9%\n",
      "Beta value: 0.00483293023857, Test accuracy: 91.4%\n",
      "Beta value: 0.00615848211066, Test accuracy: 90.7%\n",
      "Beta value: 0.00784759970351, Test accuracy: 90.2%\n",
      "Beta value: 0.01, Test accuracy: 89.7%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1068e0e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "beta_vals = np.logspace(-4, -2, 20)\n",
    "accuracy_vals = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : beta_val}\n",
    "      _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    print(\"Beta value: %s, Test accuracy: %.1f%%\" % (beta_val, accuracy(test_prediction.eval(), test_labels)))\n",
    "    accuracy_vals.append(accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "plt.semilogx(beta_vals, accuracy_vals)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (neural network)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the graph.\n",
    "\n",
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # This is the coefficient before L2 regularization \n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    ) + beta_regul * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Initialized\n",
      "Minibatch loss at step 0: 641.318115\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 20.7%\n",
      "Minibatch loss at step 500: 190.610229\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 1000: 115.596634\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 1500: 70.104088\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 2000: 42.515179\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 2500: 25.784170\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 3000: 15.638687\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.2%\n",
      "Test accuracy: 80.4%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "num_of_batches = 3  # We only feed 3 different batches into the NN.\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  print(\"Initializing\")\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_of_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the graph.\n",
    "\n",
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # This is the coefficient before L2 regularization \n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    dropout1 = tf.nn.dropout(y1, 0.5)\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    ) + beta_regul * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Initialized\n",
      "Minibatch loss at step 0: 765.695068\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 28.4%\n",
      "Minibatch loss at step 500: 199.755768\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1000: 114.147232\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1500: 68.156570\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2000: 41.798901\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2500: 25.338974\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 3000: 15.308699\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 92.6%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  print(\"Initializing\")\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Initialized\n",
      "Minibatch loss at step 0: 641.948059\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 31.4%\n",
      "Minibatch loss at step 500: 190.845871\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 1000: 115.739418\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 1500: 70.190788\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 2000: 42.567680\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 2500: 25.815784\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 3000: 15.657446\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.2%\n",
      "Test accuracy: 80.6%\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a fixed beta value.\n",
    "\n",
    "num_steps = 3001\n",
    "batch_size = 128\n",
    "num_of_batches = 3  # We only feed 3 different batches into the NN.\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  print(\"Initializing\")\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_of_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
